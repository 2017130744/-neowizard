{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled52.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3fkr4li8QJvSM6Zcvuwc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017130744/-neowizard/blob/main/19%2C20%2C21%20deep%20learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLwTCoCZ3icq"
      },
      "outputs": [],
      "source": [
        "## 입력신호를 받아 특정 값의 임계점을 넘어서는 경우에 출력을 생성해주는 함수를 활성화함수(activation function)이라고 한다. \n",
        "## logitic regression시스템의 sigmoid 함수가 대표적인 활성화함수이다. sigmoid 에서 임계점은 0.5로서, 입력값 합이 0.5보다 크면 1을 출력,ㅣ 크지 않으면 출력 x (0은 출력이 없는 것으로 간주)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 딥러닝으로 xor 문제 해결하기\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "##1. external function(sigmoid, nemirecal_derivative)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1+np.exp(-x))\n",
        "\n",
        "\n",
        "def numerical_derivative(f,x):\n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x)        \n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags = ['readwrite'])\n",
        "\n",
        "  while not it.finished:\n",
        "    idx = it.multi_index   \n",
        "\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + delta_x\n",
        "    fx1 = f(x)    ##f(x + delta_x)\n",
        "\n",
        "    x[idx] = tmp_val - delta_x\n",
        "    fx2 = f(x)   ##f(x-delta_x)\n",
        "\n",
        "    grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
        "\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad"
      ],
      "metadata": {
        "id": "muc_BqYB_IOM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. LogicGate class(__init__, __loss_func, error_val)\n",
        "\n",
        "class LogicGate:\n",
        "\n",
        "  def __init__(self, gate_name, xdata, tdata):\n",
        "\n",
        "    self.name = gate_name\n",
        "\n",
        "    #입력데이터, 정답데이터 초기화\n",
        "    self.__xdata = xdata.reshape(4,2)  #입력데이터는 (0,0), (0,1), (1,0), (1,1) 4가지이다. \n",
        "    self.__tdata = tdata.reshape(4,1)\n",
        "\n",
        "    # 2층 hidden layer unit : 6개의 노드를 가정, 가중치 W2, 바이어스 b2 초기화\n",
        "    self.__W2 = np.random.rand(2,6)   \n",
        "    self.__b2 = np.random.rand(6)\n",
        "\n",
        "    # 3층 hidden layer unit : 최종 출력값이므로 노드는 1개, 가중치 W3, 바이어스 b3 초기화\n",
        "    self.__W3 = np.random.rand(6,1)   \n",
        "    self.__b3 = np.random.rand(1)\n",
        "\n",
        "    #학습률 learning data 초기화\n",
        "    self.__learning_rate = 1e-2\n",
        "\n",
        "    print(self.name + ' object is created')\n",
        "\n",
        "    #feed forward 를 통하여 손실함수 값 계산\n",
        "  def feed_forward(self):\n",
        "\n",
        "     delta = 1e-7   ## log 무한대 발산 방지\n",
        "\n",
        "     z2 = np.dot(self.__xdata, self.__W2) + self.__b2    ##은닉층의 선형회귀 값 계산\n",
        "     a2 = sigmoid(z2)                                    ##은닉층의 출력\n",
        "\n",
        "     z3 = np.dot(a2, self.__W3) + self.__b3              ##출력층의 선형회귀 값\n",
        "     y = a3 = sigmoid(z3)                                ##출력층의 출력\n",
        "\n",
        "     #cross-entropy\n",
        "     return -np.sum(self.__tdata*np.log(y+delta) + (1-self.__tdata)*np.log((1-y)+delta))\n",
        "\n",
        "    #외부 출력을위한 손실함수 값 계산\n",
        "  def loss_val(self):\n",
        "\n",
        "     delta = 1e-7\n",
        "\n",
        "     z2 = np.dot(self.__xdata, self.__W2) + self.__b2    ##은닉층의 선형회귀 값 계산\n",
        "     a2 = sigmoid(z2)                                    ##은닉층의 출력\n",
        "\n",
        "     z3 = np.dot(a2, self.__W3) + self.__b3              ##출력층의 선형회귀 값\n",
        "     y = a3 = sigmoid(z3)                                ##출력층의 출력\n",
        "\n",
        "      #cross-entropy\n",
        "     return -np.sum(self.__tdata*np.log(y+delta) + (1-self.__tdata)*np.log((1-y)+delta))\n",
        "\n",
        "\n",
        "    #수치미분을 이용하여 손실함수가 최소가 될 때까지 학습하는 함수\n",
        "  def train(self):\n",
        "\n",
        "      f = lambda x : self.feed_forward()\n",
        "\n",
        "      print(\"initial loss value = \", self.loss_val())\n",
        "\n",
        "      for step in range(10001):\n",
        "\n",
        "        self.__W2 -= self.__learning_rate * numerical_derivative(f, self.__W2)\n",
        "        self.__b2 -= self.__learning_rate * numerical_derivative(f, self.__b2)\n",
        "        self.__W3 -= self.__learning_rate * numerical_derivative(f, self.__W3)\n",
        "        self.__b3 -= self.__learning_rate * numerical_derivative(f, self.__b3)    ##은닉층마다 그 사이에 있는 가중치와 바이어스를 모두 추가해주어야 한다. \n",
        "\n",
        "        if (step % 400 == 0):\n",
        "          print('step = ', step, 'loss value = ', self.loss_val())\n",
        "\n",
        "      \n",
        "    #미래 값 예측 함수\n",
        "  def predict (self, xdata):\n",
        "    \n",
        "     z2 = np.dot(xdata, self.__W2) + self.__b2           ##은닉층의 선형회귀 값 계산\n",
        "     a2 = sigmoid(z2)                                    ##은닉층의 출력\n",
        "\n",
        "     z3 = np.dot(a2, self.__W3) + self.__b3              ##출력층의 선형회귀 값\n",
        "     y = a3 = sigmoid(z3)                                ##출력층의 출력\n",
        "\n",
        "     if y > 0.5:\n",
        "       result = 1  #True\n",
        "      \n",
        "     else:\n",
        "       result = 0  #False\n",
        "       \n",
        "     return y, result"
      ],
      "metadata": {
        "id": "_QiQa7j7_IRN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AND Gate 객체 생성 및 training\n",
        "\n",
        "xdata = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "tdata = np.array([0,0,0,1])\n",
        "\n",
        "and_obj = LogicGate('AND', xdata, tdata)\n",
        "\n",
        "and_obj.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyrTzG-ODW1A",
        "outputId": "663d80d6-2e65-493f-d0bd-39882d05c085"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND object is created\n",
            "initial loss value =  9.370148565925152\n",
            "step =  0 loss value =  9.05739425780665\n",
            "step =  400 loss value =  2.3092721330890233\n",
            "step =  800 loss value =  2.2199348920425432\n",
            "step =  1200 loss value =  2.1077934054047227\n",
            "step =  1600 loss value =  1.938602276456208\n",
            "step =  2000 loss value =  1.6818778589674874\n",
            "step =  2400 loss value =  1.3449584588941783\n",
            "step =  2800 loss value =  1.0166984345633414\n",
            "step =  3200 loss value =  0.756139674284663\n",
            "step =  3600 loss value =  0.5641120587805265\n",
            "step =  4000 loss value =  0.4278397362766977\n",
            "step =  4400 loss value =  0.33230299278500947\n",
            "step =  4800 loss value =  0.2647645057092348\n",
            "step =  5200 loss value =  0.21607670205415047\n",
            "step =  5600 loss value =  0.18013605389452175\n",
            "step =  6000 loss value =  0.15296073965388626\n",
            "step =  6400 loss value =  0.13194483297459486\n",
            "step =  6800 loss value =  0.11535685646372792\n",
            "step =  7200 loss value =  0.10202303890307049\n",
            "step =  7600 loss value =  0.09113041419121809\n",
            "step =  8000 loss value =  0.08210386992998842\n",
            "step =  8400 loss value =  0.0745283059731654\n",
            "step =  8800 loss value =  0.06809846563882592\n",
            "step =  9200 loss value =  0.06258597396322185\n",
            "step =  9600 loss value =  0.057817259325344865\n",
            "step =  10000 loss value =  0.05365848607059931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#and gate prediction\n",
        "\n",
        "test_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "for data in test_data:\n",
        "  print(and_obj.predict(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwoukhmWE-e6",
        "outputId": "e360d564-0084-4c4e-9d21-1bec4569ef23"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0.00024615]), 0)\n",
            "(array([0.01435261]), 0)\n",
            "(array([0.01375151]), 0)\n",
            "(array([0.97520345]), 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xor gate 검증\n",
        "\n",
        "xdata = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "tdata = np.array([0,1,1,0])\n",
        "\n",
        "xor_obj = LogicGate('XOR', xdata, tdata)\n",
        "\n",
        "xor_obj.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOJPx81nIvBG",
        "outputId": "0a2509d7-7dce-48b7-e82c-01861b78ed3f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR object is created\n",
            "initial loss value =  7.8348554122121765\n",
            "step =  0 loss value =  7.673828393622834\n",
            "step =  400 loss value =  2.7688085170198757\n",
            "step =  800 loss value =  2.7653174338240123\n",
            "step =  1200 loss value =  2.761372236652676\n",
            "step =  1600 loss value =  2.7566127577955175\n",
            "step =  2000 loss value =  2.7506033961723686\n",
            "step =  2400 loss value =  2.7427860663019166\n",
            "step =  2800 loss value =  2.732429778557251\n",
            "step =  3200 loss value =  2.7185800312669612\n",
            "step =  3600 loss value =  2.7000143439393223\n",
            "step =  4000 loss value =  2.6752074051606276\n",
            "step =  4400 loss value =  2.6422974586398276\n",
            "step =  4800 loss value =  2.5990353290110506\n",
            "step =  5200 loss value =  2.5427132895607896\n",
            "step =  5600 loss value =  2.4701113222296005\n",
            "step =  6000 loss value =  2.377495484654553\n",
            "step =  6400 loss value =  2.2606323898726144\n",
            "step =  6800 loss value =  2.1149639846235617\n",
            "step =  7200 loss value =  1.9369651590237527\n",
            "step =  7600 loss value =  1.7281211218515375\n",
            "step =  8000 loss value =  1.499690558598142\n",
            "step =  8400 loss value =  1.2715690206592574\n",
            "step =  8800 loss value =  1.0633235709810998\n",
            "step =  9200 loss value =  0.8860865754304548\n",
            "step =  9600 loss value =  0.741767534654425\n",
            "step =  10000 loss value =  0.6267898154747327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xor gate prediction\n",
        "\n",
        "test_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "for data in test_data:\n",
        "  print(xor_obj.predict(data))\n",
        "\n",
        "\n",
        "  ##딥러닝을 이용하면 xor 도 해결이 가능하다. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaVo6rShI9xX",
        "outputId": "3e215ca1-7568-4abc-cebc-f8347024890d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0.04886691]), 0)\n",
            "(array([0.85623553]), 1)\n",
            "(array([0.84634137]), 1)\n",
            "(array([0.2248097]), 0)\n"
          ]
        }
      ]
    }
  ]
}