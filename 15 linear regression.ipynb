{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled49.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMooVbhQ8vfACBnKVdG+GTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017130744/-neowizard/blob/main/15%20linear%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJlYPzMA2fB7"
      },
      "outputs": [],
      "source": [
        "##손실함수(lost function)는 모든 오차의 제곱의 평균이다. 이를 E(W,b)라고 한다. W는 가중치(기울기), b는 y절편(bias)이다. 이 값이 작다는 것은 정답(t)과 y=Wx+b에 의해 계산된 값의 평균 오차가 작다는 의미.\n",
        "##이는 미지의 데이터 x가 주어질 경우 확률적으로 미래의 결과값도 오차가 작을 것이라고 추측 가능하게 함.\n",
        "##이처럼 training data를 바탕으로 손실함수 E(W,b)가 최소값을 갖도록 (W,b)를 구하는 것이 linear regression model의 최종 목적이다. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##경사하강법(gradient decent algorithm)\n",
        "##임의의 가중치 W를 선택하고 그 W에서의 직선의 기울기를 나타내는 미분값을 구한다. \n",
        "##그 미분값의 절댓값이 작아지는 방향으로 W를 감소(혹은 증가)시켜나가면 최종적으로 기울기의 절댓값이 더 이상 작아지지 않는 곳을 찾을 수 있는데, 그곳이 손실함수의 최솟값임을 알 수 있다.\n",
        "## b 값도 마찬가지로 실행한다. "
      ],
      "metadata": {
        "id": "yHZTD8pFD8BX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## simple regression 예시\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "##1. 학습데이터(traning data)준비\n",
        "\n",
        "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
        "t_data = np.array([2,3,4,5,6]).reshape(5,1)   ##원래 (x,t) 데이터는 [[1,2],[2,3],[3,4],[4,5],[5,6]] 과 같은 형태이다. \n",
        "\n",
        "##2. 임의의 직선 y=Wx+b 정의(임의의 값으로 가중치w, 바이어스b 초기화)\n",
        "W = np.random.rand(1,1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "##3. 손실함수 E(W,b) 정의\n",
        "\n",
        "def loss_func(x,t):\n",
        "  y = np.dot(x,W) + b\n",
        "\n",
        "  return (np.sum((t-y)**2) / (len(x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "N-ZWcJUqGUDB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. 수치미분 numerical_derivative 및 utility 함수 정의\n",
        "def numerical_derivative(f,x):   \n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x)        \n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags = ['readwrite'])\n",
        "\n",
        "  while not it.finished:\n",
        "    idx = it.multi_index   \n",
        "\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + delta_x\n",
        "    fx1 = f(x) \n",
        "\n",
        "    x[idx] = tmp_val - delta_x\n",
        "    fx2 = f(x)  \n",
        "\n",
        "    grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
        "\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad\n",
        "\n",
        "def error_val(x,t):                 ##손실함수 값 계산함수\n",
        "  y = np.dot(x,W) + b\n",
        "\n",
        "  return (np.sum((t-y)**2)) / (len(x))\n",
        "\n",
        "def predict(x):             ##학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
        "  y = np.dot(x,W) + b\n",
        "  return y\n",
        "\n"
      ],
      "metadata": {
        "id": "6gP9RhO_Zqi9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##5. 학습률(learning rate) 초기화 및 손실함수가 최소가 될 때까지 W,b 업데이트\n",
        "\n",
        "learning_rate = 1e-2  #발산하는 경우 le-3, le-6등으로 바꾸어서 실행\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data)\n",
        "\n",
        "print('initial error value = ', error_val(x_data,t_data), 'initial W = ', W, '\\n', 'b = ', b)\n",
        "\n",
        "for step in range(8001):\n",
        "\n",
        "  W -= learning_rate * numerical_derivative(f,W)\n",
        "  b -= learning_rate * numerical_derivative(f,b)\n",
        "\n",
        "  if (step%400 == 0):\n",
        "    print(\"step = \", step, \"error value = \", error_val(x_data, t_data), 'W = ', W, ', b = ', b)"
      ],
      "metadata": {
        "id": "IU84tFESa6uu",
        "outputId": "bf7b477e-492e-4ee7-d6a0-5b4b2338466a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial error value =  1.9018576970152292 initial W =  [[0.73371387]] \n",
            " b =  [0.472193]\n",
            "step =  0 error value =  1.1327731681346394 W =  [[0.82396523]] , b =  [0.49331123]\n",
            "step =  400 error value =  0.002270785395105811 W =  [[1.03094459]] , b =  [0.88830754]\n",
            "step =  800 error value =  0.0001448895330293062 W =  [[1.00781655]] , b =  [0.97178667]\n",
            "step =  1200 error value =  9.244808790249732e-06 W =  [[1.00197445]] , b =  [0.99287336]\n",
            "step =  1600 error value =  5.898734558761157e-07 W =  [[1.00049874]] , b =  [0.99819982]\n",
            "step =  2000 error value =  3.763741380063858e-08 W =  [[1.00012598]] , b =  [0.99954528]\n",
            "step =  2400 error value =  2.401489511846856e-09 W =  [[1.00003182]] , b =  [0.99988514]\n",
            "step =  2800 error value =  1.5322922839663235e-10 W =  [[1.00000804]] , b =  [0.99997099]\n",
            "step =  3200 error value =  9.776930658375895e-12 W =  [[1.00000203]] , b =  [0.99999267]\n",
            "step =  3600 error value =  6.238259768471802e-13 W =  [[1.00000051]] , b =  [0.99999815]\n",
            "step =  4000 error value =  3.980378534100333e-14 W =  [[1.00000013]] , b =  [0.99999953]\n",
            "step =  4400 error value =  2.5397168635297754e-15 W =  [[1.00000003]] , b =  [0.99999988]\n",
            "step =  4800 error value =  1.6204896264677342e-16 W =  [[1.00000001]] , b =  [0.99999997]\n",
            "step =  5200 error value =  1.0339679684075944e-17 W =  [[1.]] , b =  [0.99999999]\n",
            "step =  5600 error value =  6.597330457484338e-19 W =  [[1.]] , b =  [1.]\n",
            "step =  6000 error value =  4.209485117144481e-20 W =  [[1.]] , b =  [1.]\n",
            "step =  6400 error value =  2.6859180584042595e-21 W =  [[1.]] , b =  [1.]\n",
            "step =  6800 error value =  1.7138129936860026e-22 W =  [[1.]] , b =  [1.]\n",
            "step =  7200 error value =  1.0936118357459111e-23 W =  [[1.]] , b =  [1.]\n",
            "step =  7600 error value =  6.982844877292141e-25 W =  [[1.]] , b =  [1.]\n",
            "step =  8000 error value =  4.475740396429824e-26 W =  [[1.]] , b =  [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##미래값 예측\n",
        "\n",
        "predict(43)"
      ],
      "metadata": {
        "id": "V5aAHYlabXjB",
        "outputId": "5e15e0f3-b00f-4b05-d3c0-b5e44845f3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##입력 변수가 여러개인 multi_variable regression\n",
        "\n",
        "## y = x1w1 + x2w2 + x3w3 + b 와 같은 형태. 이 때 x * W + b = y 에서 x는 변수가 3개이므로 nx3행렬로 나타난다. w도 가중치가 3개이므로 1x3행렬이다. \n",
        "\n",
        "## simple regression에서 두 번째 과정에서 W = np.random.rand(3,1)로 해주어서 3개의 가중치들이 임의의 값을 가지도록 하면된다. 나머지는 모두 같다. \n"
      ],
      "metadata": {
        "id": "L7UKqn43cwK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}