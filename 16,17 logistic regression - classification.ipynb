{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaqYEv6GL087m8plReuJr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017130744/-neowizard/blob/main/16%2C17%20logistic%20regression%20-%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUINpHBr4r6p"
      },
      "outputs": [],
      "source": [
        "## Logistic Regression 알고리즘\n",
        "## 1. Training data 특성과 분포를 나타내는 최적의 직선을 찾고(linear regression)\n",
        "## 2. 그 직선을 기준으로 데이터를 위(1) 또는 아래(0) 등으로 분류(classification)해주는 알고리즘\n",
        "## 이러한 logistic regression은 classification 알고리즘 중에서도 정확도가 높은 알고리즘으로 알려져 있어서 deep learning에서 기본 component로 사용됨\n",
        "## (x,t) -> regression -> classification -> true(1) or false(0)\n",
        "\n",
        "## sigmoid function: 함수값으로 0~1 사이의 값만을 가지는 함수. \n",
        "## 출력값 y가 1 또는 0만을 가져야 하는 분류 시스템에서 sigmoid 함수가 사용됨. \n",
        "## 출력함수로 sigmoid를 사용해서 sigmoid 계산 값이 0.5보다 크면 결과로 1이 나올 확률이 높다는 것이기 때문에 출력값 y는 1을 정의.\n",
        "## 만약 sigmoid 계산 값이 0.5 미만이면 출력값 y는 0으로 정의. 이렇게 classification 시스템을 구현.\n",
        "## sigmoid 함수의 실제 계산 값 sigmoid(z)는 결과가 나타날 확률을 의미한다. ex) 0.75 -> 75퍼센트로 1이 출력.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression 구현하기\n",
        "import numpy as np\n",
        "\n",
        "# 1. slicing 또는 list comprehension 등을 이용하여 입력 x와 정답 t를 numpy 타입으로 분리. t=0또는t=1\n",
        "\n",
        "x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(10,1)\n",
        "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(10,1)\n",
        "\n",
        "# 2. z = Wx + b에 들어갈 W와 b를 정의. \n",
        "\n",
        "W = np.random.rand(1,1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "# 3. 손실함수 E(W,b) 정의\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1+np.exp(-x))\n",
        "\n",
        "def loss_func(x,t):\n",
        "\n",
        "  delta = 1e-7   #log 무한대 발산 방지\n",
        "\n",
        "  z = np.dot(x,W) + b\n",
        "  y = sigmoid(z)\n",
        "\n",
        "  return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))"
      ],
      "metadata": {
        "id": "-bK2r0TxTj1m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 수치미분 numerical_derivative 및 utility  함수 정의\n",
        "\n",
        "def numerical_derivative(f,x):    \n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x)         \n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags = ['readwrite']) \n",
        "\n",
        "  while not it.finished:\n",
        "    idx = it.multi_index   \n",
        "\n",
        "    tmp_val = x[idx]\n",
        "    x[idx] = float(tmp_val) + delta_x\n",
        "    fx1 = f(x)   \n",
        "\n",
        "    x[idx] = tmp_val - delta_x\n",
        "    fx2 = f(x)\n",
        "\n",
        "    grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
        "\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad\n",
        "\n",
        "def error_val(x,t):\n",
        "  delta = 1e-7\n",
        "\n",
        "  z = np.dot(x,W) + b\n",
        "  y = sigmoid(z)\n",
        "\n",
        "  return  -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
        "\n",
        "def predict(x):\n",
        "\n",
        "  z = np.dot(x,W) + b\n",
        "  y = sigmoid(z)\n",
        "\n",
        "  if y > 0.5:\n",
        "    result = 1\n",
        "  \n",
        "  else:\n",
        "    result = 0\n",
        "\n",
        "  return y, result\n"
      ],
      "metadata": {
        "id": "0JqlNDojXyK0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 학습률(learning rate) 초기화 및 손실함수가 최소가 될 때까지 W,b 업데이트\n",
        "\n",
        "learning_rate = 1e-3  #발산하는 경우 le-3, le-6등으로 바꾸어서 실행\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data)\n",
        "\n",
        "print('initial error value = ', error_val(x_data,t_data), 'initial W = ', W, '\\n', 'b = ', b)\n",
        "\n",
        "for step in range(10001):\n",
        "\n",
        "  W -= learning_rate * numerical_derivative(f,W)\n",
        "  b -= learning_rate * numerical_derivative(f,b)\n",
        "\n",
        "  if (step%400 == 0):\n",
        "    print(\"step = \", step, \"error value = \", error_val(x_data, t_data), 'W = ', W, ', b = ', b)"
      ],
      "metadata": {
        "id": "2uaRK_BOYUaJ",
        "outputId": "7bdcb019-5d4d-4485-d83f-c70f6878b9b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial error value =  0.20962533018435026 initial W =  [[2.21930306]] \n",
            " b =  [-28.76384783]\n",
            "step =  0 error value =  0.2096250841948307 W =  [[2.21930426]] , b =  [-28.76386347]\n",
            "step =  400 error value =  0.20952673503710897 W =  [[2.21978488]] , b =  [-28.77011772]\n",
            "step =  800 error value =  0.20942847885671761 W =  [[2.22026527]] , b =  [-28.77636902]\n",
            "step =  1200 error value =  0.2093303155180067 W =  [[2.22074543]] , b =  [-28.78261736]\n",
            "step =  1600 error value =  0.20923224488560133 W =  [[2.22122537]] , b =  [-28.78886275]\n",
            "step =  2000 error value =  0.20913426682440023 W =  [[2.22170508]] , b =  [-28.79510519]\n",
            "step =  2400 error value =  0.20903638119957088 W =  [[2.22218457]] , b =  [-28.80134469]\n",
            "step =  2800 error value =  0.2089385878765557 W =  [[2.22266383]] , b =  [-28.80758124]\n",
            "step =  3200 error value =  0.20884088672106704 W =  [[2.22314286]] , b =  [-28.81381486]\n",
            "step =  3600 error value =  0.20874327759908737 W =  [[2.22362167]] , b =  [-28.82004554]\n",
            "step =  4000 error value =  0.20864576037686722 W =  [[2.22410025]] , b =  [-28.82627328]\n",
            "step =  4400 error value =  0.20854833492092895 W =  [[2.22457861]] , b =  [-28.83249809]\n",
            "step =  4800 error value =  0.20845100109806197 W =  [[2.22505675]] , b =  [-28.83871997]\n",
            "step =  5200 error value =  0.20835375877532214 W =  [[2.22553466]] , b =  [-28.84493893]\n",
            "step =  5600 error value =  0.20825660782003252 W =  [[2.22601234]] , b =  [-28.85115497]\n",
            "step =  6000 error value =  0.2081595480997826 W =  [[2.2264898]] , b =  [-28.85736809]\n",
            "step =  6400 error value =  0.2080625794824254 W =  [[2.22696704]] , b =  [-28.86357828]\n",
            "step =  6800 error value =  0.2079657018360809 W =  [[2.22744406]] , b =  [-28.86978557]\n",
            "step =  7200 error value =  0.20786891502913385 W =  [[2.22792085]] , b =  [-28.87598994]\n",
            "step =  7600 error value =  0.2077722189302278 W =  [[2.22839741]] , b =  [-28.88219141]\n",
            "step =  8000 error value =  0.20767561340827498 W =  [[2.22887376]] , b =  [-28.88838997]\n",
            "step =  8400 error value =  0.20757909833244378 W =  [[2.22934988]] , b =  [-28.89458563]\n",
            "step =  8800 error value =  0.2074826735721676 W =  [[2.22982578]] , b =  [-28.90077839]\n",
            "step =  9200 error value =  0.20738633899713987 W =  [[2.23030146]] , b =  [-28.90696825]\n",
            "step =  9600 error value =  0.20729009447731225 W =  [[2.23077691]] , b =  [-28.91315522]\n",
            "step =  10000 error value =  0.2071939398828974 W =  [[2.23125214]] , b =  [-28.91933929]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(12)"
      ],
      "metadata": {
        "id": "1KJBx70uZGck",
        "outputId": "6bdff519-9ddb-4652-ddbc-02dac31c19a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.10600526]]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##multi-variable logistic regression (classification)\n",
        "## 변수가 2개 이상인 것이다. 따라서 입력행렬과 가중치 행렬이 (변수가 2개, 데이터가 n개라면) nx2, 2x1 이다. \n",
        "## 학습데이터 준비, 임의의 직선 정의를 내리는 것이 이전에 linear regression에서 multi-value를 하던것 처럼 하면 된다. "
      ],
      "metadata": {
        "id": "eBfRywNEdCKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}