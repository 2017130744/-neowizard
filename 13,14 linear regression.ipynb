{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled49.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqFLwV+5zl55gqbjMHfwd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2017130744/-neowizard/blob/main/13%2C14%20linear%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJlYPzMA2fB7"
      },
      "outputs": [],
      "source": [
        "##손실함수(lost function)는 모든 오차의 제곱의 평균이다. 이를 E(W,b)라고 한다. W는 가중치(기울기), b는 y절편(bias)이다. 이 값이 작다는 것은 정답(t)과 y=Wx+b에 의해 계산된 값의 평균 오차가 작다는 의미.\n",
        "##이는 미지의 데이터 x가 주어질 경우 확률적으로 미래의 결과값도 오차가 작을 것이라고 추측 가능하게 함.\n",
        "##이처럼 training data를 바탕으로 손실함수 E(W,b)가 최소값을 갖도록 (W,b)를 구하는 것이 linear regression model의 최종 목적이다. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##경사하강법(gradient decent algorithm)\n",
        "##임의의 가중치 W를 선택하고 그 W에서의 직선의 기울기를 나타내는 미분값을 구한다. \n",
        "##그 미분값의 절댓값이 작아지는 방향으로 W를 감소(혹은 증가)시켜나가면 최종적으로 기울기의 절댓값이 더 이상 작아지지 않는 곳을 찾을 수 있는데, 그곳이 손실함수의 최솟값임을 알 수 있다.\n",
        "## b 값도 마찬가지로 실행한다. "
      ],
      "metadata": {
        "id": "yHZTD8pFD8BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N-ZWcJUqGUDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}